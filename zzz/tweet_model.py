# -*- coding: utf-8 -*-
"""tweet_model.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Swhj0aEySBmYkX0oJm_8URPdCOVmVSUP
"""

#pip install datasets
#https://medium.com/@hsiuchun/nlp-實戰教學-bert-情緒分析-中-bef4900c62f8



from datasets import load_dataset
dataset = load_dataset("imdb")

dataset

dataset['train'][0]

import pandas as pd

all_data = [] # a list to save all data

for data in dataset['train']:
  all_data.append({'text':data['text'], 'label':data['label']})
for data in dataset['test']:
  all_data.append({'text':data['text'], 'label':data['label']})

all_df = pd.DataFrame(all_data, columns=['text', 'label'])
all_df.head(5)

print(all_df.label.value_counts() / len(all_df))
'''[Output]
0    0.5
1    0.5
Name: label, dtype: float64
'''

# 使用 sklearn 的套件直接幫我們切（如果還未安裝 sklearn 的話，在命令列執行 "!pip install sklearn" 即可）
from sklearn.model_selection import train_test_split

# random_state 是固定資料 random 的結果，才不會每次切出來的資料集不一樣喔~
# train_size：指定 output 中前者資料數量占比
parameters = {'seed': 42}  # 這裡設定種子為 42

train_df, temp_data = train_test_split(all_df, random_state=parameters['seed'], train_size=0.8)
val_df, test_df = train_test_split(temp_data, random_state=parameters['seed'], train_size=0.5)
print('# of train_df:', len(train_df))
print('# of val_df:', len(val_df))
print('# of test_df data:', len(test_df))

# save data
# 這裡指定sep='\t'，且不儲存DataFrame前面的index
train_df.to_csv('./train.tsv', sep='\t', index=False)
val_df.to_csv('./val.tsv', sep='\t', index=False)
test_df.to_csv('./test.tsv', sep='\t', index=False)

#!pip install transformers

from transformers import AutoTokenizer
import transformers

# 如果你發現在使用時常常有一長串的 warning 跳出來，可以用這行指令把它關掉
# transformers.logging.set_verbosity_error() # Close the warning message

config_name = 'bert-base-uncased' # 假設我們用 bert（base是比較少層的模型，uncased是不調整大小寫）
# .from_pretrained() 就是用現有的模型繼續做
tokenizer = AutoTokenizer.from_pretrained(config_name)

sample_s = "How's everything going?"

# tokenize，通常會採用空白/標點切字（也可以自己切好再做轉換，需改參數設定）
token = tokenizer.tokenize(sample_s)
print(token)
'''[Output]
['how', "'", 's', 'everything', 'going', '?']
'''

# encode，將文字轉為數字（透過該 tokenzier 的 vocab 去做轉換）
# 什麼參數都沒改的話，會自動幫你加上 [CLS] 和 [SEP] （以 BERT 來說）
ids = tokenizer.encode(sample_s)
print(ids)
'''[Output]
[101, 2129, 1005, 1055, 2673, 2183, 1029, 102]
'''

# decode，將數字轉回文本
tokenizer.decode(ids)
'''[Output]
[CLS] how's everything going? [SEP]
'''

# 純粹去對 vocab 做轉換
print(tokenizer.convert_ids_to_tokens(ids))
'''[Output]
['[CLS]', 'how', "'", 's', 'everything', 'going', '?', '[SEP]']
'''

# 純粹對單詞做轉換
print(tokenizer.convert_tokens_to_ids(token))
'''[Output]
[2129, 1005, 1055, 2673, 2183, 1029]
'''

# 將 token list 中的所有元素使用空白做 join
print(tokenizer.convert_tokens_to_string(token))
'''[Output]
how ' s everything going?
'''

# 不改任何參數
sample_s = "How's everything going?"
es = tokenizer.encode_plus(sample_s)
print(es)
'''[Output]
{
 'input_ids': [101, 2129, 1005, 1055, 2673, 2183, 1029, 102],
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0],
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]
}
'''

# 固定文本長度
## truncated
sample_s = "How's everything going?"
es = tokenizer.encode_plus(
    sample_s,
    max_length = 7,
    truncation = True,
    padding = 'max_length'
)
print(es)
'''[Output]
{
 'input_ids': [101, 2129, 1005, 1055, 2673, 2183, 102],
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0],
 'attention_mask': [1, 1, 1, 1, 1, 1, 1]
}
'''

## padding
sample_os = "How are you?"
os = tokenizer.encode_plus(
    sample_os,
    max_length = 7,
    truncation = True,
    padding = 'max_length'
)
print(os)
'''[Output]
{
 'input_ids': [101, 2129, 2024, 2017, 1029, 102, 0],
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0],
 'attention_mask': [1, 1, 1, 1, 1, 1, 0]
}
'''

# 回傳 tensor 型態
es = tokenizer.encode_plus(
    sample_s,
    max_length = 7,
    truncation = True,
    padding = 'max_length',
    return_tensors = 'pt'
)
print(es)
'''[Output]
{
 'input_ids': tensor([[ 101, 2129, 1005, 1055, 2673, 2183,  102]]),
 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]),
 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])
}
'''

import torch
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer
import pandas as pd
import torch.nn.functional as Fun

# Using Dataset to build DataLoader
class CustomDataset(Dataset):
    def __init__(self, mode, df, specify, args):
        assert mode in ["train", "val", "test"]  # 一般會切三份
        self.mode = mode
        self.df = df
        self.specify = specify # specify column of data (the column U use for predict)
        if self.mode != 'test':
          self.label = df['label']
        self.tokenizer = AutoTokenizer.from_pretrained(args.get("config"))
        self.max_len = args.get("max_len", 512)
        self.num_class = args.get("num_class", 2)

    def __len__(self):
        return len(self.df)

    # transform label to one_hot label (if num_class > 2)
    def one_hot_label(self, label):
        return Fun.one_hot(torch.tensor(label), num_classes=self.num_class)

    # transform text to its number
    def tokenize(self,input_text):
        inputs = self.tokenizer.encode_plus(
            input_text,
            max_length=self.max_len,
            truncation=True,
            padding='max_length'
        )
        ids = inputs['input_ids'] # (512)
        mask = inputs['attention_mask'] # (512)
        token_type_ids = inputs["token_type_ids"] # (512)

        return ids, mask, token_type_ids

    # get single data
    def __getitem__(self, index):

        sentence = str(self.df[self.specify][index])
        ids, mask, token_type_ids = self.tokenize(sentence)

        if self.mode == "test":
            return torch.tensor(ids, dtype=torch.long), torch.tensor(mask, dtype=torch.long), \
                torch.tensor(token_type_ids, dtype=torch.long)
        else:
            # 回傳 input_ids, attention_mask, totken_type_ids, labels
            # 需回傳 tensor 型態，其維度為 torch.Size([self.max_len])
            if self.num_class > 2:
                return torch.tensor(ids, dtype=torch.long), torch.tensor(mask, dtype=torch.long), \
                    torch.tensor(token_type_ids, dtype=torch.long), self.one_hot_label(self.label[index])
            else:
                return torch.tensor(ids, dtype=torch.long), torch.tensor(mask, dtype=torch.long), \
                    torch.tensor(token_type_ids, dtype=torch.long), torch.tensor(self.label[index], dtype=torch.long)

# 定義參數
parameters = {
    "config": "bert-base-uncased",
    "max_len": 512,
    "num_class": 2,
    "batch_size": 32,
    "seed": 42
}

# load training data
train_df = pd.read_csv('./train.tsv', sep='\t').sample(4000, random_state=parameters['seed']).reset_index(drop=True)
train_dataset = CustomDataset('train', train_df, 'text', parameters)
train_loader = DataLoader(train_dataset, batch_size=parameters['batch_size'], shuffle=True)

# load validation data
val_df = pd.read_csv('./val.tsv', sep='\t').sample(500, random_state=parameters['seed']).reset_index(drop=True)
val_dataset = CustomDataset('val', val_df, 'text', parameters)
val_loader = DataLoader(val_dataset, batch_size=parameters['batch_size'], shuffle=True)

"""上面是資料處理

"""

from transformers.models.bert.modeling_bert import BertPreTrainedModel, BertModel
import torch.nn as nn

# BERT Model
class BertClassifier(BertPreTrainedModel):
    def __init__(self, config, args):
        super(BertClassifier, self).__init__(config)
        self.bert = BertModel(config)
        self.num_labels = args["num_class"]
        self.dropout = nn.Dropout(args["dropout"])
        self.classifier = nn.Linear(config.hidden_size, self.num_labels)
        self.init_weights()

    # forward function, data in model will do this
    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, position_ids=None,
                head_mask=None, inputs_embeds=None, labels=None, output_attentions=None,
                output_hidden_states=None, return_dict=None):

        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        # bert output
        outputs = self.bert(
            input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
            position_ids=position_ids,
            head_mask=head_mask,
            inputs_embeds=inputs_embeds,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict
        )

        # get its [CLS] logits
        pooled_output = outputs[1]
        pooled_output = self.dropout(pooled_output) # add dropout
        logits = self.classifier(pooled_output) # add linear classifier

        return logits

import torch.nn as nn
import copy

# define different activation function
def get_activation(activation):
    if activation == 'Prelu':
        return nn.PReLU()
    elif activation == 'relu':
        return nn.ReLU()
    elif activation == 'sigmoid':
        return nn.Sigmoid()
    elif activation == 'gelu':
        return nn.GELU()
    elif activation == 'LeakyReLU':
        return nn.LeakyReLU()
    else:
        return nn.Tanh()
# Dense Layer
# It is composed of linear, dropout, and activation layers.
class Dense(nn.Module):
    def __init__(self, input_dim, output_dim, dropout_rate, activation='tanh'):
        super(Dense, self).__init__()
        self.hidden_layer = nn.Linear(input_dim, output_dim)
        self.dropout = nn.Dropout(dropout_rate)
        self.activation = get_activation(activation) # default tanh
        nn.init.xavier_uniform_(self.hidden_layer.weight) # you also can change the initialize method
    def forward(self, inputs):
        logits = self.hidden_layer(inputs)
        logits = self.dropout(logits)
        logits = self.activation(logits)
        return logits
# multi-layers
def _get_clones(module, N):
    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])
# Hidden Layers
# It means there are many dense layers with the same dimension
class HiddenLayers(nn.Module):
    def __init__(self, dense_layer, num_layers):
        super(HiddenLayers, self).__init__()
        self.hidden_layers = _get_clones(dense_layer, num_layers)
    def forward(self, output):
        for layer in self.hidden_layers:
            output = layer(output)
        return output

from transformers.models.bert.modeling_bert import BertPreTrainedModel, BertModel
import torch.nn as nn

# BERT Model
class BertClassifier(BertPreTrainedModel):
    def __init__(self, config, args):
        super(BertClassifier, self).__init__(config)
        self.bert = BertModel(config)
        self.num_labels = args["num_class"]
        self.dense = Dense(config.hidden_size, args["hidden_dim"], args["dropout"], args["activation"])
        self.classifier = Dense(args["hidden_dim"], self.num_labels, args["dropout"], args["activation"])
        self.init_weights()
    # forward function, data in the model will do this
    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, position_ids=None,
                head_mask=None, inputs_embeds=None, labels=None, output_attentions=None,
                output_hidden_states=None, return_dict=None):
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
        # bert output
        outputs = self.bert(
            input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
            position_ids=position_ids,
            head_mask=head_mask,
            inputs_embeds=inputs_embeds,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict
        )
        '''
        outputs.keys() -> odict_keys(['last_hidden_state', 'pooler_output'])
        outs.last_hidden_state.shape -> torch.Size([batch_size, 512, 768])
        outs.pooler_output.shape -> torch.Size([batch_size, 768])
        '''
        # get its [CLS] logits
        pooled_output = outputs[1] # (batch_size, 768)
        # add dense layer
        pooled_output = self.dense(pooled_output) # (batch_size, 384)
        # add linear classifier
        logits = self.classifier(pooled_output) # (batch_size, 2)
        return logits

from transformers import AutoModel

# 創建一個預訓練的 Transformer 模型
model = AutoModel.from_pretrained('bert-base-uncased')

# 獲取模型配置
config = model.config

# 獲取隱藏狀態大小
hidden_size = config.hidden_size

print("Hidden State Size:", hidden_size)

from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score# get predict result

def get_pred(logits):
    y_pred = torch.argmax(logits, dim = 1)
    return y_pred

# calculate confusion metrics
def cal_metrics(pred, ans, method):
    '''
    Parameter
    ---------
    pred: [list], predict class
    ans: [list], true class
    method: 'micro', 'weighted', 'macro'. # 如果有多分類的話計算上會有差別
    ---------
    '''
    if pred.get_device() != 'cpu':
        pred = pred.detach().cpu().numpy()
    if ans.get_device() != 'cpu':
        ans = ans.detach().cpu().numpy()
    # 將 zero_division 設為 0，表示當所有預測皆錯誤時，將結果視為 0
    rec = recall_score(pred, ans, average=method, zero_division=0)
    f1 = f1_score(pred, ans, average=method, zero_division=0)
    prec = precision_score(pred, ans, average=method, zero_division=0)
    acc = accuracy_score(pred, ans)
    return acc, f1, rec, prec

from datetime import datetime
parameters = {
    "num_class": 2,
    "time": str(datetime.now()).replace(" ", "_"), # I like to annotate when I trained
    "seed": 1111,
    # Hyperparameters
    "model_name": 'BERT', # If U have a lot of different models, it is easy for U to know what it is
    "config": 'bert-base-uncased', # which pre-trained model config U use
    "learning_rate": 1e-4, # the speed that model learn
    "epochs": 3, # If U would fine-tune it, the epochs didn't need to set too much
    "max_len": 512, # the max length of input tokens in the BERT model
    "batch_size": 16,
    "dropout": 0.1, # how random amount will be give up
    "activation": 'tanh',
    "hidden_dim": 384,
}

import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = BertClassifier.from_pretrained(parameters['config'], parameters).to(device)
loss_fct = nn.CrossEntropyLoss() # we use cross entrophy loss

## You can custom your optimizer (e.g. SGD .etc) ##
# we use Adam here
optimizer = torch.optim.Adam(model.parameters(), lr=parameters['learning_rate'], betas=(0.9, 0.999), eps=1e-9)

## You also can add your custom scheduler ##
# num_train_steps = len(train_loader) * parameters['epochs]
# scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=int(0.1 * num_train_steps), num_training_steps=num_train_steps, num_cycles=1)

import torch.nn as nn
# evaluate dataloader
def evaluate(model, data_loader, device):
    val_loss, val_acc, val_f1, val_rec, val_prec = 0.0, 0.0, 0.0, 0.0, 0.0
    step_count = 0
    loss_fct = nn.CrossEntropyLoss()
    model.eval()
    with torch.no_grad():
        for data in data_loader:
            ids, masks, token_type_ids, labels = [t.to(device) for t in data]

            logits = model(input_ids = ids,
                    token_type_ids = token_type_ids,
                    attention_mask = masks)
            acc, f1, rec, prec = cal_metrics(get_pred(logits), labels, 'macro')
            loss = loss_fct(logits, labels) # 直接丟就好，不用特意做轉換（但如果非二分類，需考慮 one-hot 標籤的轉換）

            val_loss += loss.item()
            val_acc += acc
            val_f1 += f1
            val_rec += rec
            val_prec += prec
            step_count+=1

        val_loss = val_loss / step_count
        val_acc = val_acc / step_count
        val_f1 = val_f1 / step_count
        val_rec = val_rec / step_count
        val_prec = val_prec / step_count

    return val_loss, val_acc, val_f1, val_rec, val_prec

import time
def train(model, train_loader, val_loader, optimizer, args, device):

    metrics = ['loss', 'acc', 'f1', 'rec', 'prec']
    mode = ['train_', 'val_']
    record = {s+m :[] for s in mode for m in metrics}

    loss_fct = nn.CrossEntropyLoss()

    for epoch in range(args["epochs"]):

        st_time = time.time()
        train_loss, train_acc, train_f1, train_rec, train_prec = 0.0, 0.0, 0.0, 0.0, 0.0
        step_count = 0

        model.train()
        for data in train_loader:

            ids, masks, token_type_ids, labels = [t.to(device) for t in data]

            optimizer.zero_grad()

            logits = model(input_ids = ids,
                    token_type_ids = token_type_ids,
                    attention_mask = masks)

            acc, f1, rec, prec = cal_metrics(get_pred(logits), labels, 'macro')
            loss = loss_fct(logits, labels)

            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            train_acc += acc
            train_f1 += f1
            train_rec += rec
            train_prec += prec
            step_count += 1

        val_loss, val_acc, val_f1, val_rec, val_prec = evaluate(model, val_loader, device)

        train_loss = train_loss / step_count
        train_acc = train_acc / step_count
        train_f1 = train_f1 / step_count
        train_rec = train_rec / step_count
        train_prec = train_prec / step_count

        print('[epoch %d] cost time: %.4f s'%(epoch + 1, time.time() - st_time))
        print('         loss     acc     f1      rec    prec')
        print('train | %.4f, %.4f, %.4f, %.4f, %.4f'%(train_loss, train_acc, train_f1, train_rec, train_prec))
        print('val  | %.4f, %.4f, %.4f, %.4f, %.4f\n'%(val_loss, val_acc, val_f1, val_rec, val_prec))

        # record training metrics of each training epoch
        record['train_loss'].append(train_loss)
        record['train_acc'].append(train_acc)
        record['train_f1'].append(train_f1)
        record['train_rec'].append(train_rec)
        record['train_prec'].append(train_prec)

        record['val_loss'].append(val_loss)
        record['val_acc'].append(val_acc)
        record['val_f1'].append(val_f1)
        record['val_rec'].append(val_rec)
        record['val_prec'].append(val_prec)

    # save model
    save_checkpoint(args["model_name"] + '_' + args["time"].split('_')[0] + '.pt', model)

    return record

# save model to path
def save_checkpoint(save_path, model):
    if save_path == None:
        return
    torch.save(model.state_dict(), save_path)
    print(f'Model saved to ==> {save_path}')

# load model from path
def load_checkpoint(load_path, model, device):
    if load_path==None:
        return
    state_dict = torch.load(load_path, map_location=device)
    print(f'\nModel loaded from <== {load_path}')

    model.load_state_dict(state_dict)
    return model

import matplotlib.pyplot as plt

# draw the learning curve
def draw_pic(record, name, img_save=False, show=False):
    x_ticks = range(1, parameters['epochs']+1)

    plt.figure(figsize=(6, 3))

    plt.plot(x_ticks, record['train_'+name], '-o', color='lightskyblue',
             markeredgecolor="teal", markersize=3, markeredgewidth=1, label = 'Train')
    plt.plot(x_ticks, record['val_'+name], '-o', color='pink',
             markeredgecolor="salmon", markersize=3, markeredgewidth=1, label = 'Val')
    plt.grid(color='lightgray', linestyle='--', linewidth=1)

    plt.title('Model', fontsize=14)
    plt.ylabel(name, fontsize=12)
    plt.xlabel('Epoch', fontsize=12)
    plt.xticks(x_ticks, fontsize=12)
    plt.yticks(fontsize=12)
    plt.legend(loc='lower right' if not name.lower().endswith('loss') else 'upper right')

    # define saved figure or not
    if img_save:
        plt.savefig(name+'.png', transparent=False, dpi=300)
    if show:
        plt.show()

    plt.close()

history = train(model, train_loader, val_loader, optimizer, parameters, device)

# draw all metrics figure
draw_pic(history, 'loss', img_save=True, show=False)
draw_pic(history, 'acc', img_save=True, show=False)
draw_pic(history, 'f1', img_save=True, show=False)
draw_pic(history, 'rec', img_save=True, show=False)
draw_pic(history, 'prec', img_save=True, show=False)

files = []
files.append('loss.png')
files.append('acc.png')
files.append('f1.png')
files.append('rec.png')
files.append('prec.png')
send_email(parameters, files)

