# -*- coding: utf-8 -*-
"""LSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1y4soy--EofXHK_7oZgnk0bVigvZLl4jC
"""

!pip install scikeras
!pip install keras-tuner

import os
import keras
import warnings
import numpy as np
import pandas as pd
import yfinance as yf
import keras_tuner as kt
from sklearn import metrics
import matplotlib.pyplot as plt
from keras.models import Sequential
from pandas_datareader import data as pdr
from keras.callbacks import EarlyStopping
from keras_tuner.tuners import RandomSearch
from scikeras.wrappers import KerasRegressor
from keras.layers import Dense, LSTM, Dropout
from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import LogisticRegression
from keras_tuner.engine.hyperparameters import HyperParameters
from sklearn.model_selection import GridSearchCV, train_test_split

# Commented out IPython magic to ensure Python compatibility.
warnings.filterwarnings("ignore")
# %config InlineBackend.figure_format = 'retina'

from google.colab import drive
drive.mount('/content/drive')

os.chdir('/content/drive/MyDrive/GDSC AI Stock')

## Yahoo Finance for TSLA and VIX

yf.pdr_override()
df = pdr.get_data_yahoo("TSLA", start = "2015-01-01", end = "2023-12-31").reset_index()
vix = pdr.get_data_yahoo("^VIX", start = "2015-01-01", end = "2023-12-31").reset_index()

fin = pd.read_csv('./Financial Statement.csv')

df['Date'] = pd.to_datetime(df['Date'])
vix['Date'] = pd.to_datetime(vix['Date'])
fin['Date'] = pd.to_datetime(fin['Date'])

df.drop(columns = ['Adj Close'], inplace = True)
vix = vix[['Date', 'Close']]
fin = fin[['Date', 'Gross Margin (YoY%)', 'Operating Margin (YoY%)', 'Quick Ratio (YoY%)', 'EPS Growth (USD)']]
df['Delta'] = round(df['Close'].pct_change()*100, 4)
df.dropna(inplace = True)

df = pd.merge(left = df, right = vix, left_on = 'Date', right_on = 'Date', suffixes = ('_TSLA', '_VIX'), how = 'left')
df = pd.merge(left = df, right = fin, left_on = 'Date', right_on = 'Date', how = 'left')
df.set_index('Date', inplace = True)

def custom_ffill(series):

    previous_value = None
    day = 0

    for i, value in series.items():

        if pd.isna(value):
            day += 1
            if previous_value is not None:

                series[i] = previous_value / 2
                previous_value = previous_value / 2

                # if abs(previous_value) <= float(1e-4):
                if day > 14:
                    previous_value = 0
        else:
            previous_value = value
            day = 0

    return series

for c in df:
    df[c] = custom_ffill(df[c])

df.dropna(inplace = True)
df.reset_index(inplace = True)

df['Open_Close'] = 0.0
df['High_Low'] = 0.0

for i in range(len(df)):
    df['Open_Close'][i] = (df['Open'][i] - df['Close_TSLA'][i]) * 100 / df['Open'][i]
    df['High_Low'][i] = (df['High'][i] - df['Low'][i]) * 100 / df['Low'][i]

df['Target'] = df['Close_TSLA'].shift(-1)
df.dropna(inplace = True)

features = df[["Close_TSLA", "Volume", "Close_VIX", "Gross Margin (YoY%)", "Operating Margin (YoY%)", "Quick Ratio (YoY%)", "EPS Growth (USD)", "Open_Close", "High_Low"]]
target = df['Target']

scaler = MinMaxScaler()
features_scaled = scaler.fit_transform(features)
target_scaled = scaler.fit_transform(target.values.reshape(-1, 1))

num_train = int(len(features_scaled) * 0.9)

x_train = features_scaled[:num_train]
x_test = features_scaled[num_train:]

y_train = target_scaled[:num_train]
y_test = target_scaled[num_train:]

x_train = np.reshape(x_train, (x_train.shape[0], 1, x_train.shape[1]))
x_test = np.reshape(x_test, (x_test.shape[0], 1, x_test.shape[1]))

x_train.shape, x_test.shape

"""### RandomSearch"""

def build_model(hp):
    model = Sequential()

    model.add(
        LSTM(hp.Int('input_unit', min_value = 32, max_value = 512, step = 32), return_sequences = True, input_shape = (x_train.shape[1], x_train.shape[2]))
        )

    for i in range(hp.Int('n_layers', 1, 4)):
        model.add(
            LSTM(hp.Int(f'lstm_{i}_units', min_value = 32, max_value = 512, step = 32), return_sequences = True)
            )

    model.add(
        Dropout(hp.Float('Dropout_rate_1', min_value = 0, max_value = 0.5, step = 0.1))
        )

    model.add(
        LSTM(hp.Int('layer_2_neurons', min_value = 32, max_value = 512, step = 32))
        )

    model.add(
        Dropout(hp.Float('Dropout_rate_2', min_value = 0, max_value = 0.5, step = 0.1))
        )

    # model.add(Dense(25))
    model.add(Dense(hp.Int('dense_1_units', min_value = 32, max_value = 512, step = 32)))
    model.add(Dense(1))

    model.compile(loss = 'mean_squared_error', optimizer = 'adam', metrics = ['mse'])

    return model

tuner = RandomSearch(
        build_model,
        objective = 'val_mse',
        max_trials = 10,
        executions_per_trial = 1,
        overwrite = True
)

tuner.search(
        x = x_train,
        y = y_train,
        epochs = 20,
        batch_size = 32,
        validation_data = (x_test, y_test)
)

best_model = tuner.get_best_models(num_models = 1)[0]
best_model.summary()

loss = best_model.evaluate(x_test, y_test)
print(f'Mean Squared Error on Test Data: {round(loss[0], 4)}')

x_new = np.reshape(features_scaled[-1], (1, 1, features_scaled.shape[1]))
predicted_scaled = best_model.predict(x_new)
predicted = scaler.inverse_transform(predicted_scaled)
print(f'Predicted Close_TSLA for the next day: {predicted[0][0]}')

### Predict for testing data

predictions = best_model.predict(x_test)
predictions = scaler.inverse_transform(predictions)

train = df[['Close_TSLA']][:num_train]
valid = df[['Close_TSLA']][num_train:]
valid['Predictions'] = predictions

plt.figure(figsize = (16,6))
plt.title('Model')
plt.xlabel('Date', fontsize = 18)
plt.ylabel('Close Price USD ($)', fontsize = 18)
# plt.plot(train['Close_TSLA'])
plt.plot(valid[['Close_TSLA', 'Predictions']])
plt.legend(['Train', 'Val', 'Predictions'], loc = 'lower right')
plt.show()

### Simply predict for next 7 day

result = []
for i in range(7):
    predicted_scaled = best_model.predict(x_new)
    x_new[0, 0, 0] = predicted_scaled[0, 0]
    predicted = scaler.inverse_transform(predicted_scaled)
    result.append(predicted[0, 0])

result